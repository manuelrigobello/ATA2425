{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVANCED TEXT ANALYTICS 2024/2025\n",
    "\n",
    "## Scope of the project\n",
    "Starting from a pre-trained model, the goal of the project is to attach a trained [ner](https://spacy.io/api/entityrecognizer) component to the model such that it will recognize labels coming from the medical field. The code is based on the spaCy Python library ([documentation here](https://spacy.io/api/doc)).\n",
    "\n",
    "To address the [\"catastrophic forgetting\" problem](https://en.wikipedia.org/wiki/Catastrophic_interference), the trained ner component will be attached to a pre-trained model, the same one used for training the component, so that the output of the model will contain labels that can be assigned either by the original ner or by the trained ner component. Another possible solution could be performing a [\"rehease\"](https://spacy.io/api/language#rehearse), but in this project it is not explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "\n",
    "### STEP 0: install required libraries and check  for the GPU\n",
    "Remove the comments to install the libraries required for running this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf\n",
    "\n",
    "# Probably it is required to restart the Jupyter kernel after this instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations for NVIDIA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install cupy-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0 []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check version of tensorflow and if GPU is available\n",
    "print(tf.__version__, tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download required models\n",
    "\n",
    "The models used in this project are:\n",
    "- `en_core_web_lg`: English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer. Docs: [en_core_web_lg](https://spacy.io/models/en#en_core_web_lg)\n",
    "- `en_core_web_trf`: English transformer pipeline (Transformer(name=‘roberta-base’, piece_encoder=‘byte-bpe’, stride=104, type=‘roberta’, width=768, window=144, vocab_size=50265)). Components: transformer, tagger, parser, ner, attribute_ruler, lemmatizer. Docs: [en_core_web_trf](https://spacy.io/models/en#en_core_web_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "\n",
    "### STEP 1: prepare training set and dev set\n",
    "\n",
    "In this project we define the **Annotations** that contains the annotations of the articles, and the **Articles** that contains the articles.\n",
    "\n",
    "To upload the data use the following logic:\n",
    "- Store the Annotations Train data inside `./Annotations/Train` folder;\n",
    "- Store the Annotations Dev data inside `./Annotations/Dev` folder.\n",
    "- Store the Articles Train data inside `./Articles/Train` folder;\n",
    "- Store the Articles Dev data inside `./Articles/Dev` folder.\n",
    "\n",
    "[DocBin](https://spacy.io/api/docbin) is used to store and serialize the Doc objects. The train DocBin will be saved in the `./TrainDocBin/train.spacy` file and the dev DocBin in the `./DevDocBin/dev.spacy` file.\n",
    "\n",
    "**Note:** go to [step 2](#step2.0) if you already have the train and dev set well formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "path = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "\n",
    "# print(path)\n",
    "\n",
    "# Open training set\n",
    "with open(path + '/Articles/Train/articles_train.txt','r', encoding='UTF-8') as articlesTrainFile:\n",
    "  articlesTrain = articlesTrainFile.read().split('\\n\\n')\n",
    "  # Remove last empty line if present\n",
    "  if articlesTrain[len(articlesTrain)-1] == '\\n':\n",
    "    articlesTrain = articlesTrain[:len(articlesTrain)-1]\n",
    "\n",
    "with open(path + '/Annotations/Train/entities_train.txt','r', encoding='UTF-8') as entitiesTrainFile:\n",
    "  entitiesTrainFile.readline()\n",
    "  entitiesTrain = entitiesTrainFile.read().split('\\n\\n')\n",
    "  # Remove last empty line if present\n",
    "  if entitiesTrain[len(entitiesTrain)-1] == '\\n':\n",
    "    entitiesTrain = entitiesTrain[:len(entitiesTrain)-1]\n",
    "\n",
    "with open(path + '/Articles/Dev/articles_dev.txt','r', encoding='UTF-8') as articlesDevFile:\n",
    "  articlesDev = articlesDevFile.read().split('\\n\\n')\n",
    "  if articlesDev[len(articlesDev)-1] == '\\n':\n",
    "    articlesDev = articlesDev[:len(articlesDev)-1]\n",
    "\n",
    "with open(path + '/Annotations/Dev/entities_dev.txt','r', encoding='UTF-8') as entitiesDevFile:\n",
    "  entitiesDevFile.readline()\n",
    "  entitiesDev = entitiesDevFile.read().split('\\n\\n')\n",
    "  if entitiesDev[len(entitiesDev)-1] == '\\n':\n",
    "    entitiesDev = entitiesDev[:len(entitiesDev)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_article(text):\n",
    "  article = re.findall(r'a\\|(.*)',text)\n",
    "  return article[0]\n",
    "\n",
    "def get_title(text):\n",
    "  # print(text)\n",
    "  title = re.findall(r't\\|(.*)',text)\n",
    "  return title[0]\n",
    "\n",
    "def get_pmid(text):\n",
    "  pmid = text.split('|', 1)[0]\n",
    "  pmid = re.sub('\\n', '', pmid)\n",
    "  return pmid\n",
    "\n",
    "def calc_article_start(text):\n",
    "  title = re.findall(r't\\|(.*)',text)\n",
    "  return len(title[0]) + 1 # +1 because of space char added between title and abstract\n",
    "\n",
    "# Articles for train data\n",
    "train_id = [get_pmid(x) for x in articlesTrain]\n",
    "train_articles = [get_title(x)+' '+get_article(x) for x in articlesTrain]\n",
    "train_articles_start_at = [calc_article_start(x) for x in articlesTrain]\n",
    "\n",
    "# Articles for test data\n",
    "dev_id = [get_pmid(x) for x in articlesDev]\n",
    "dev_articles = [get_title(x)+' '+get_article(x) for x in articlesDev]\n",
    "dev_articles_start_at = [calc_article_start(x) for x in articlesDev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(columns=['article', 'articleStartsAt'])\n",
    "train_df['pmid'] = train_id\n",
    "train_df['article'] = train_articles\n",
    "train_df['articleStartsAt'] = train_articles_start_at\n",
    "\n",
    "dev_df = pd.DataFrame(columns=['article'])\n",
    "dev_df['pmid'] = dev_id\n",
    "dev_df['article'] = dev_articles\n",
    "dev_df['articleStartsAt'] = dev_articles_start_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train labels:  1566 , total test labels:  40\n"
     ]
    }
   ],
   "source": [
    "def get_labels(dataframe, text):\n",
    "  text = text.strip() # Remove possible \\n at the start/end of the text\n",
    "  l = text.split(\"\\n\")\n",
    "  l = [x.split('\\t') for x in l]\n",
    "  labels = []\n",
    "  index = 0\n",
    "  for i in l:\n",
    "    # print(i)\n",
    "    while dataframe.iloc[index]['pmid'] != i[0]:\n",
    "      index += 1\n",
    "      continue\n",
    "\n",
    "    if i[4] == 'title':\n",
    "      labels.append((int(i[2]), int(i[3])+1, i[6]))\n",
    "    elif i[4] == 'abstract':\n",
    "      # Add shift due to the title length\n",
    "      labels.append((int(i[2]) + int(dataframe.iloc[index]['articleStartsAt']), int(i[3]) + 1 + int(dataframe.iloc[index]['articleStartsAt']), i[6]))\n",
    "  return labels\n",
    "\n",
    "train_labels = [get_labels(train_df, x) for x in entitiesTrain]\n",
    "dev_labels = [get_labels(dev_df, x) for x in entitiesDev]\n",
    "\n",
    "print('total train labels: ', len(train_labels), ', total test labels: ' , len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['labels'] = train_labels\n",
    "dev_df['labels'] = dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for i, j in zip(train_articles, train_labels):\n",
    "  training_data.append((i, j))\n",
    "\n",
    "# print(training_data[0])\n",
    "\n",
    "dev_data = []\n",
    "for i, j in zip(dev_articles, dev_labels):\n",
    "  dev_data.append((i, j))\n",
    "\n",
    "# print(dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy.training\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1.1'></a>\n",
    "\n",
    "### STEP 1.1: improve Tokenizer\n",
    "\n",
    "Sometimes a word followed by a punctuation (or viceversa) are not considered as two distinct tokens. To fix this, a custom tokenizer is implemented by adding the list of puctuations to the default prefixes and suffixes lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing the tokenizer\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "  my_punct = f\"[{re.escape(string.punctuation)}]\"\n",
    "  my_punct = my_punct.replace(\"<\", \"\")\n",
    "  my_punct = my_punct.replace(\">\", \"\")\n",
    "  html_tag_pattern = re.compile(r'<[^>]+>')\n",
    "\n",
    "  all_prefixes_re = spacy.util.compile_prefix_regex(tuple(list(nlp.Defaults.prefixes) + [html_tag_pattern.pattern] + [my_punct]))\n",
    "  infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n",
    "  suffix_re = spacy.util.compile_suffix_regex(tuple(list(nlp.Defaults.suffixes) + [html_tag_pattern.pattern] + [my_punct]))\n",
    "\n",
    "  \n",
    "  return Tokenizer(nlp.vocab,\n",
    "                   nlp.Defaults.tokenizer_exceptions,\n",
    "                   prefix_search = all_prefixes_re.search, \n",
    "                   infix_finditer = infix_re.finditer,\n",
    "                   suffix_search = suffix_re.search,\n",
    "                   token_match=None)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated version of the tokenizer\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "  prefix_re = re.compile(r'[.,;:?!(\\[\\'\"</]')\n",
    "  suffix_re = re.compile(r'[.,;:?!)\\]\\'\">■™]')\n",
    "  infix_re = re.compile(r'[-/+=&]')\n",
    "\n",
    "  return Tokenizer(\n",
    "        nlp.vocab,\n",
    "        prefix_search=prefix_re.search,\n",
    "        suffix_search=suffix_re.search,\n",
    "        infix_finditer=infix_re.finditer\n",
    "    )\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "def save_to_disk(data, dir, filename):\n",
    "  db = DocBin()\n",
    "  for text, annotations in data:\n",
    "    doc = nlp(text)\n",
    "    # sentence_tokens = []\n",
    "    # for sent in doc.sents:\n",
    "    #   sentence_tokens.append([token.text for token in sent])\n",
    "    # print(sentence_tokens)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "      span = doc.char_span(int(start), int(end), label=label)\n",
    "      if not span == None:\n",
    "        ents.append(span)\n",
    "      else:\n",
    "        print(\"none\", int(start), int(end), label)\n",
    "    # print(ents)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "  db.to_disk(os.path.join(dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none 1529 1547 bacteria\n",
      "none 92 95 gene\n",
      "none 579 596 anatomical location\n",
      "none 167 170 DDF\n",
      "none 359 381 human\n",
      "none 788 791 DDF\n",
      "none 806 828 human\n",
      "none 979 986 human\n",
      "none 1543 1554 DDF\n"
     ]
    }
   ],
   "source": [
    "save_to_disk(training_data, os.path.join(path,'TrainDocBin'), \"updated_tokenizer_train.spacy\")\n",
    "save_to_disk(dev_data, os.path.join(path,'DevDocBin'), \"updated_tokenizer_dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "\n",
    "### STEP 2: prepare CUDA and PyTorch\n",
    "\n",
    "If your PC is already set up correctly, then skip to [step 3](#step3).\n",
    "\n",
    "#### Check if CUDA is available\n",
    "The instruction *torch.cuda.is_available()* checks if CUDA is avaiable for running the train on the GPU.\n",
    "If the answer if false, then it means either PyTorch or CUDA or both of them is not installed.\n",
    "\n",
    "#### Install PyTorch\n",
    "To install PyTorch, go to [this link](https://pytorch.org/get-started/locally/), select your preferences (in this case it is important to set a CUDA version as \"Compute Platform\" so that the code will run on the GPU) and then copy-paste the command into the following cell.\n",
    "\n",
    "It might be necessary to restart the runtime.\n",
    "\n",
    "After installing pythorch, *torch.cuda.is_available()* returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "import torch\n",
    "\n",
    "print('is cuda available? ', torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "\n",
    "### STEP 3: train the NER component\n",
    "\n",
    "#### Generate config.cfg file\n",
    "Generate the base_config.cfg configuration file that includes all the settings and hyperparameters.\n",
    "In this project the focus is to train only the ner component.\n",
    "The train will be optimized for accuracy over efficiency.\n",
    "Then, save the config to config.cfg file\n",
    "\n",
    "For this project the training is done with an NVIDIA GeForce 4060 laptop with 8GB of VRAM. \n",
    "\n",
    "#### Train en_core_web_trf NER component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config\\trf\\config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manuel\\ata-project\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Manuel\\ata-project\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ./config/trf/base_config.cfg ./config/trf/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manuel\\ata-project\\Scripts\\python.exe: No module named prodigy\n"
     ]
    }
   ],
   "source": [
    "# !python -m prodigy spacy-config config.cfg --ner custom_tok_annotations --base-model custom_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import spacy.training\n",
    "\n",
    "# # Load pre-trained model over which a new training for the ner component will be done\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "output\\lg-improved-tokenizer-custom-config\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    137.43    0.00    0.00    0.00    0.00\n",
      "  0     150       1056.41   8241.94   37.83   62.47   27.13    0.38\n",
      "  0     300        943.52   5278.64   19.17   18.52   19.87    0.19\n",
      "  0     450       3744.66   5849.71   56.92   63.51   51.57    0.57\n",
      "  0     600        400.30   4585.79   60.93   68.07   55.15    0.61\n",
      "  0     750        305.83   4226.44   67.11   66.12   68.13    0.67\n",
      "  0     900        317.47   4046.03   61.07   75.83   51.12    0.61\n",
      "  0    1050       1144.42   4130.05   65.78   77.04   57.39    0.66\n",
      "  0    1200        419.58   3934.65   69.76   74.00   65.98    0.70\n",
      "  0    1350       1249.25   3637.42   64.26   62.47   66.16    0.64\n",
      "  0    1500        418.32   3553.06   66.03   71.03   61.68    0.66\n",
      "  1    1650        493.54   3462.78   70.66   74.02   67.59    0.71\n",
      "  1    1800        381.06   3277.63   72.44   78.27   67.41    0.72\n",
      "  1    1950        640.18   3402.15   69.25   68.85   69.65    0.69\n",
      "  1    2100        586.20   3190.65   71.04   74.01   68.31    0.71\n",
      "  1    2250        791.03   3417.35   70.79   75.53   66.61    0.71\n",
      "  1    2400        673.86   3709.57   74.22   75.82   72.69    0.74\n",
      "  1    2550        574.34   3284.76   70.65   70.30   70.99    0.71\n",
      "  1    2700        467.49   2949.99   73.80   73.47   74.13    0.74\n",
      "  1    2850        612.42   3349.00   72.87   78.91   67.68    0.73\n",
      "  1    3000       1802.77   3288.71   72.98   75.80   70.37    0.73\n",
      "  2    3150        625.01   3315.22   73.85   75.73   72.07    0.74\n",
      "  2    3300        762.29   2929.25   73.54   76.80   70.55    0.74\n",
      "  2    3450       1701.73   3479.53   73.25   73.08   73.41    0.73\n",
      "  2    3600        551.62   3023.50   72.02   74.76   69.47    0.72\n",
      "  2    3750        623.86   2667.76   71.55   73.26   69.92    0.72\n",
      "  2    3900        756.53   3069.36   75.04   78.68   71.71    0.75\n",
      "  2    4050        864.09   3064.11   71.75   74.07   69.56    0.72\n",
      "  2    4200        783.66   3038.88   75.42   79.54   71.71    0.75\n",
      "  2    4350        929.69   2910.54   74.55   74.89   74.22    0.75\n",
      "  2    4500        694.90   2675.76   71.67   73.81   69.65    0.72\n",
      "  2    4650        841.85   2958.06   72.71   76.05   69.65    0.73\n",
      "  3    4800        593.06   2575.28   74.47   78.09   71.17    0.74\n",
      "  3    4950        766.27   2563.58   73.19   71.77   74.66    0.73\n",
      "  3    5100        626.87   2255.88   74.90   76.84   73.05    0.75\n",
      "  3    5250        797.73   2589.13   74.76   71.81   77.98    0.75\n",
      "  3    5400       1016.56   2777.42   76.94   80.19   73.95    0.77\n",
      "  3    5550        825.27   2605.76   74.59   75.34   73.86    0.75\n",
      "  3    5700       1059.01   2762.06   72.78   77.53   68.58    0.73\n",
      "  3    5850       1265.67   2948.42   73.99   74.12   73.86    0.74\n",
      "  3    6000        787.18   2701.91   72.44   72.18   72.69    0.72\n",
      "  3    6150        770.03   2504.48   74.66   75.48   73.86    0.75\n",
      "  4    6300        929.25   2927.76   77.21   78.17   76.28    0.77\n",
      "  4    6450       1026.28   2553.97   73.83   76.79   71.08    0.74\n",
      "  4    6600        980.23   2604.59   76.22   77.75   74.75    0.76\n",
      "  4    6750       1358.80   2843.94   76.29   79.51   73.32    0.76\n",
      "  4    6900        938.04   2467.46   78.22   78.64   77.80    0.78\n",
      "  4    7050        703.11   2550.37   76.49   76.09   76.90    0.76\n",
      "  4    7200       1066.49   2406.04   76.18   74.96   77.44    0.76\n",
      "  4    7350       1194.05   2222.37   76.64   75.61   77.71    0.77\n",
      "  4    7500       1114.82   2440.90   73.05   72.34   73.77    0.73\n",
      "  4    7650        994.98   2501.32   74.12   72.90   75.38    0.74\n",
      "  4    7800       1654.50   2608.67   78.27   79.68   76.90    0.78\n",
      "  5    7950        926.62   2316.32   77.87   77.94   77.80    0.78\n",
      "  5    8100       1075.05   2111.76   76.09   78.37   73.95    0.76\n",
      "  5    8250       1685.00   2554.32   75.11   75.38   74.84    0.75\n",
      "  5    8400       1113.70   2476.12   77.78   78.97   76.63    0.78\n",
      "  5    8550       1140.05   2383.04   78.73   79.78   77.71    0.79\n",
      "  5    8700       1176.89   2583.13   77.98   79.28   76.72    0.78\n",
      "  5    8850       1112.98   2305.15   76.38   75.77   76.99    0.76\n",
      "  5    9000       1206.89   2336.31   76.14   75.73   76.54    0.76\n",
      "  5    9150       1262.97   2424.17   75.02   77.61   72.61    0.75\n",
      "  5    9300       1320.18   2317.12   77.27   78.49   76.10    0.77\n",
      "  6    9450       1200.38   2386.74   78.67   80.32   77.08    0.79\n",
      "  6    9600       1966.56   2463.57   76.36   80.20   72.87    0.76\n",
      "  6    9750       1462.15   2198.29   77.63   79.22   76.10    0.78\n",
      "  6    9900       1277.70   2328.08   77.27   76.06   78.51    0.77\n",
      "  6   10050       1269.82   2164.50   77.71   82.43   73.50    0.78\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output\\lg-improved-tokenizer-custom-config\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manuel\\ata-project\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Manuel\\ata-project\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config/lg/custom-config.cfg --output ./output/lg-improved-tokenizer-custom-config --paths.train ./TrainDocBin/my_tokenizer_train.spacy --paths.dev ./DevDocBin/my_tokenizer_dev.spacy --code ./custom-tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "\n",
    "for example in dev_data:\n",
    "\tentities = example[1]\n",
    "\tfor entity in entities:\n",
    "\t\tentity_label = entity[2]\n",
    "\t\tunique_labels.add(entity_label)\n",
    "        \n",
    "unique_labels_list = list(unique_labels)\n",
    "\n",
    "print('Entities to be recognised in the provided data: ')\n",
    "print(unique_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   79.78 \n",
      "NER R   77.71 \n",
      "NER F   78.73 \n",
      "SPEED   6631  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                            P       R       F\n",
      "DDF                     88.35   82.06   85.09\n",
      "bacteria                62.69   77.78   69.42\n",
      "microbiome              90.08   92.91   91.47\n",
      "human                   87.78   91.86   89.77\n",
      "anatomical location     89.39   77.63   83.10\n",
      "chemical                61.05   80.15   69.31\n",
      "biomedical technique    66.67   55.56   60.61\n",
      "statistical technique   33.33   66.67   44.44\n",
      "gene                    63.64   17.95   28.00\n",
      "animal                  84.42   89.04   86.67\n",
      "dietary supplement      67.86   70.37   69.09\n",
      "food                     0.00    0.00    0.00\n",
      "drug                    70.69   68.33   69.49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manuel\\ata-project\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Manuel\\ata-project\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "!spacy evaluate ./output/lg-improved-tokenizer-custom-config/model-best/ ./DevDocBin/my_tokenizer_dev.spacy --code custom-tokenizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "\n",
    "### STEP 3: attach the trained NER comopnent to the original model\n",
    "In the following cell, the trained NER component will be integrated into the original en_core_web_trf model. This combination will allow the final model to label words using a set that includes both the original labels and the newly trained ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "# Load trained model\n",
    "trained_nlp = spacy.load('./output/model-best')\n",
    "\n",
    "trained_nlp.replace_listeners(\"transformer\", \"ner\", [\"model.tok2vec\"])\n",
    "\n",
    "nlp.add_pipe(\n",
    "  \"ner\",\n",
    "  name=\"trained_ner\",\n",
    "  source=trained_nlp,\n",
    "  before=\"ner\",\n",
    ")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.displacy\n",
    "\n",
    "\n",
    "doc = nlp(\"Doc here\")\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 4: train using the en_core_web_lg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL: output of best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Pipeline | .spacy creation | Params | TOK | NER P | NER R | NER F |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| en_core_web_trf | Base | Base | 99.90 | 83.58 | 80.20 | 81.86 |\n",
    "| en_core_web_lg | Base | Base | 99.90 | 81.62 | 72.73 | 76.92 |\n",
    "| en_core_web_lg | improved tokenizer | Base | 99.74 | 78.29 | 74.57 | 76.39 |\n",
    "| en_core_web_trf | updated tokenizer | Base + change config | 90.71 | 80.98 | 81.56 | 81.27 |\n",
    "| en_core_web_trf | improved tokenizer | Base + change config | 99.74 | 78.89 | 83.97 | 81.35 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ata-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
