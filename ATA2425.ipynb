{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVANCED TEXT ANALYTICS 2024/2025\n",
    "\n",
    "## Scope of the project\n",
    "Starting from a pre-trained model, the goal of the project is to attach a trained [ner](https://spacy.io/api/entityrecognizer) component to the model such that it will recognize labels coming from the medical field. The code is based on the spaCy Python library ([documentation here](https://spacy.io/api/doc)).\n",
    "\n",
    "To address the [\"catastrophic forgetting\" problem](https://en.wikipedia.org/wiki/Catastrophic_interference), the trained ner component will be attached to a pre-trained model, the same one used for training the component, so that the output of the model will contain labels that can be assigned either by the original ner or by the trained ner component. Another possible solution could be performing a [\"rehease\"](https://spacy.io/api/language#rehearse), but in this project it is not explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "\n",
    "### STEP 0: install required libraries and check  for the GPU\n",
    "Remove the comments to install the libraries required for running this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf\n",
    "\n",
    "# Probably it is required to restart the Jupyter kernel after this instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations for NVIDIA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install cupy-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check version of tensorflow and if GPU is available\n",
    "print(tf.__version__, tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download required pipelines\n",
    "\n",
    "The models used in this project are:\n",
    "- `en_core_web_lg`: English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer. Docs: [en_core_web_lg](https://spacy.io/models/en#en_core_web_lg)\n",
    "- `en_core_web_trf`: English transformer pipeline (Transformer(name=‘roberta-base’, piece_encoder=‘byte-bpe’, stride=104, type=‘roberta’, width=768, window=144, vocab_size=50265)). Components: transformer, tagger, parser, ner, attribute_ruler, lemmatizer. Docs: [en_core_web_trf](https://spacy.io/models/en#en_core_web_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "\n",
    "### STEP 1: prepare training set and dev set\n",
    "\n",
    "In this project we define the **Annotations** that contains the annotations of the articles, and the **Articles** that contains the articles.\n",
    "\n",
    "To upload the data use the following logic:\n",
    "- Store the Annotations Train data inside `./Annotations/Train` folder;\n",
    "- Store the Annotations Dev data inside `./Annotations/Dev` folder.\n",
    "- Store the Articles Train data inside `./Articles/Train` folder;\n",
    "- Store the Articles Dev data inside `./Articles/Dev` folder.\n",
    "\n",
    "[DocBin](https://spacy.io/api/docbin) is used to store and serialize the Doc objects. The train DocBin will be saved in the `./TrainDocBin/train.spacy` file and the dev DocBin in the `./DevDocBin/dev.spacy` file.\n",
    "\n",
    "**Note:** go to [step 2](#step2.0) if you already have the train and dev set well formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "path = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "\n",
    "# print(path)\n",
    "\n",
    "# Open training set\n",
    "with open(path + '/Articles/Train/articles_train.txt','r', encoding='UTF-8') as articlesTrainFile:\n",
    "  articlesTrain = articlesTrainFile.read().split('\\n\\n')\n",
    "  # Remove last empty line if present\n",
    "  if articlesTrain[len(articlesTrain)-1] == '\\n':\n",
    "    articlesTrain = articlesTrain[:len(articlesTrain)-1]\n",
    "\n",
    "with open(path + '/Annotations/Train/entities_train.txt','r', encoding='UTF-8') as entitiesTrainFile:\n",
    "  entitiesTrainFile.readline()\n",
    "  entitiesTrain = entitiesTrainFile.read().split('\\n\\n')\n",
    "  # Remove last empty line if present\n",
    "  if entitiesTrain[len(entitiesTrain)-1] == '\\n':\n",
    "    entitiesTrain = entitiesTrain[:len(entitiesTrain)-1]\n",
    "\n",
    "with open(path + '/Articles/Dev/articles_dev.txt','r', encoding='UTF-8') as articlesDevFile:\n",
    "  articlesDev = articlesDevFile.read().split('\\n\\n')\n",
    "  if articlesDev[len(articlesDev)-1] == '\\n':\n",
    "    articlesDev = articlesDev[:len(articlesDev)-1]\n",
    "\n",
    "with open(path + '/Annotations/Dev/entities_dev.txt','r', encoding='UTF-8') as entitiesDevFile:\n",
    "  entitiesDevFile.readline()\n",
    "  entitiesDev = entitiesDevFile.read().split('\\n\\n')\n",
    "  if entitiesDev[len(entitiesDev)-1] == '\\n':\n",
    "    entitiesDev = entitiesDev[:len(entitiesDev)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_article(text):\n",
    "  article = re.findall(r'a\\|(.*)',text)\n",
    "  return article[0]\n",
    "\n",
    "def get_title(text):\n",
    "  # print(text)\n",
    "  title = re.findall(r't\\|(.*)',text)\n",
    "  return title[0]\n",
    "\n",
    "def get_pmid(text):\n",
    "  pmid = text.split('|', 1)[0]\n",
    "  pmid = re.sub('\\n', '', pmid)\n",
    "  return pmid\n",
    "\n",
    "def calc_article_start(text):\n",
    "  title = re.findall(r't\\|(.*)',text)\n",
    "  return len(title[0]) + 1 # +1 because of space char added between title and abstract\n",
    "\n",
    "# Articles for train data\n",
    "train_id = [get_pmid(x) for x in articlesTrain]\n",
    "train_articles = [get_title(x)+' '+get_article(x) for x in articlesTrain]\n",
    "train_articles_start_at = [calc_article_start(x) for x in articlesTrain]\n",
    "\n",
    "# Articles for test data\n",
    "dev_id = [get_pmid(x) for x in articlesDev]\n",
    "dev_articles = [get_title(x)+' '+get_article(x) for x in articlesDev]\n",
    "dev_articles_start_at = [calc_article_start(x) for x in articlesDev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(columns=['article', 'articleStartsAt'])\n",
    "train_df['pmid'] = train_id\n",
    "train_df['article'] = train_articles\n",
    "train_df['articleStartsAt'] = train_articles_start_at\n",
    "\n",
    "dev_df = pd.DataFrame(columns=['article'])\n",
    "dev_df['pmid'] = dev_id\n",
    "dev_df['article'] = dev_articles\n",
    "dev_df['articleStartsAt'] = dev_articles_start_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataframe, text):\n",
    "  text = text.strip() # Remove possible \\n at the start/end of the text\n",
    "  l = text.split(\"\\n\")\n",
    "  l = [x.split('\\t') for x in l]\n",
    "  labels = []\n",
    "  index = 0\n",
    "  for i in l:\n",
    "    # print(i)\n",
    "    while dataframe.iloc[index]['pmid'] != i[0]:\n",
    "      index += 1\n",
    "      continue\n",
    "\n",
    "    if i[4] == 'title':\n",
    "      labels.append((int(i[2]), int(i[3])+1, i[6]))\n",
    "    elif i[4] == 'abstract':\n",
    "      # Add shift due to the title length\n",
    "      labels.append((int(i[2]) + int(dataframe.iloc[index]['articleStartsAt']), int(i[3]) + 1 + int(dataframe.iloc[index]['articleStartsAt']), i[6]))\n",
    "  return labels\n",
    "\n",
    "train_labels = [get_labels(train_df, x) for x in entitiesTrain]\n",
    "dev_labels = [get_labels(dev_df, x) for x in entitiesDev]\n",
    "\n",
    "print('total train labels: ', len(train_labels), ', total test labels: ' , len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['labels'] = train_labels\n",
    "dev_df['labels'] = dev_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1.1'></a>\n",
    "\n",
    "### STEP 1.1: set tag removal from the articles and entities\n",
    "\n",
    "Set the variable `REMOVE_TAGS` to `True` to remove tags from the articles and update the start and end indexes of the entities accordingly; set to `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_TAGS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text_and_realign_entities(text, entities):\n",
    "  # Step 1: Remove all HTML tags from the main text\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  clean_text = soup.get_text()\n",
    "\n",
    "  new_entities = []\n",
    "  entity_position = {}\n",
    "\n",
    "  # Get all the starting positions for each entity\n",
    "  for entity in entities:\n",
    "    entity_soup = BeautifulSoup(text[entity[0]:entity[1]], 'html.parser')\n",
    "    clean_text_span = entity_soup.get_text().strip()\n",
    "\n",
    "    # Skip empty spans\n",
    "    if not clean_text_span:\n",
    "      print('Empty span:', text[entity[0]:entity[1]])\n",
    "      continue\n",
    "\n",
    "    entity_position[clean_text_span] = [match.start() for match in re.finditer(r'(?<!\\w)'+re.escape(clean_text_span)+r'(?!\\w)', clean_text)]\n",
    "\n",
    "  # Remove starting position that are inside other entities\n",
    "  for key in entity_position.keys():\n",
    "    others = [other for other in entity_position.keys() if key in other and other not in key] # get all the keys containing key as substring\n",
    "    starts = entity_position[key]\n",
    "\n",
    "    for other in others:\n",
    "      other_starts = entity_position[other]\n",
    "      for index in starts:\n",
    "        for other_index in other_starts:\n",
    "          if index >= other_index and index <= other_index + len(other):\n",
    "            starts.remove(index)\n",
    "              \n",
    "  # Update entity indexes\n",
    "  for entity in entities:\n",
    "    entity_soup = BeautifulSoup(text[entity[0]:entity[1]], 'html.parser')\n",
    "    clean_text_span = entity_soup.get_text().strip()\n",
    "    \n",
    "    if len(entity_position[clean_text_span]) == 0:\n",
    "      print(f\"Warning: Span '{clean_text_span}' not found in text.\")\n",
    "      continue\n",
    "    start_idx = entity_position[clean_text_span].pop(0)\n",
    "    end_idx = start_idx + len(clean_text_span)\n",
    "    new_entities.append((start_idx, end_idx, entity[2]))\n",
    "\n",
    "  return clean_text, new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_TAGS:\n",
    "  print(\"Remove tags\")\n",
    "  cleaned_train_df = pd.DataFrame(columns=['article', 'labels'])\n",
    "  cleaned_dev_df = pd.DataFrame(columns=['article', 'labels'])\n",
    "\n",
    "  print(\"Clean training set\")\n",
    "  articles = []\n",
    "  labels = []\n",
    "  for row in train_df.iloc:\n",
    "    article, label = clean_text_and_realign_entities(row['article'], row['labels'])\n",
    "    articles.append(article)\n",
    "    labels.append(label)\n",
    "\n",
    "  cleaned_train_df['article'] = articles\n",
    "  cleaned_train_df['labels'] = labels\n",
    "\n",
    "  print(\"Clean dev set\")\n",
    "  articles = []\n",
    "  labels = []\n",
    "  for row in dev_df.iloc:\n",
    "    article, label = clean_text_and_realign_entities(row['article'], row['labels'])\n",
    "    articles.append(article)\n",
    "    labels.append(label)\n",
    "\n",
    "  cleaned_dev_df['article'] = articles\n",
    "  cleaned_dev_df['labels'] = labels\n",
    "\n",
    "  train_df = cleaned_train_df\n",
    "  dev_df = cleaned_dev_df\n",
    "else:\n",
    "  print(\"No remove tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for i, j in zip(train_df['article'], train_df['labels']):\n",
    "  training_data.append((i, j))\n",
    "\n",
    "# print(training_data[0])\n",
    "\n",
    "dev_data = []\n",
    "for i, j in zip(dev_df['article'], dev_df['labels']):\n",
    "  dev_data.append((i, j))\n",
    "\n",
    "# print(dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy.training\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1.2'></a>\n",
    "\n",
    "### STEP 1.2: improve Tokenizer\n",
    "\n",
    "Sometimes a word followed by a punctuation (or viceversa) are not considered as two distinct tokens. To fix this, two different custom tokenizers are implemented:\n",
    "- the `IMPROVE` tokenizer starts from the default tokenizer and adds some regex pattern to recognize the punctuations and the HTML tags in the prefixes and suffixes of a token\n",
    "- the `UPDATE` tokenizer creates a new tokenizer and consider the most used punctuation characters and HTML tags as prefixes and suffixes, while for the infix it considers only the regex `[-/+=&]`\n",
    "\n",
    "Set the variable `WHICH_DOC_TOKENIZER` to one of the possible values to use a particular tokenizer when creating the .spacy files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing the tokenizer\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "WHICH_DOC_TOKENIZER = \"IMPROVE\" # Possible values: DEFAULT, IMPROVE, UPDATE\n",
    "\n",
    "def improve_tokenizer(nlp):\n",
    "  my_punct = f\"[{re.escape(string.punctuation)}]\"\n",
    "  my_punct = my_punct.replace(\"<\", \"\")\n",
    "  my_punct = my_punct.replace(\">\", \"\")\n",
    "  html_tag_pattern = re.compile(r'<[^>]+>')\n",
    "\n",
    "  all_prefixes_re = spacy.util.compile_prefix_regex(tuple(list(nlp.Defaults.prefixes) + [html_tag_pattern.pattern] + [my_punct]))\n",
    "  infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n",
    "  suffix_re = spacy.util.compile_suffix_regex(tuple(list(nlp.Defaults.suffixes) + [html_tag_pattern.pattern] + [my_punct]))\n",
    "\n",
    "  \n",
    "  return Tokenizer(nlp.vocab,\n",
    "                  nlp.Defaults.tokenizer_exceptions,\n",
    "                  prefix_search = all_prefixes_re.search, \n",
    "                  infix_finditer = infix_re.finditer,\n",
    "                  suffix_search = suffix_re.search,\n",
    "                  token_match=None)\n",
    "\n",
    "def update_tokenizer(nlp):\n",
    "  prefix_re = re.compile(r'[.,;:?!(\\[\\'\"</]')\n",
    "  suffix_re = re.compile(r'[.,;:?!)\\]\\'\">■™]')\n",
    "  infix_re = re.compile(r'[-/+=&]')\n",
    "\n",
    "  return Tokenizer(nlp.vocab,\n",
    "                  prefix_search=prefix_re.search,\n",
    "                  suffix_search=suffix_re.search,\n",
    "                  infix_finditer=infix_re.finditer)\n",
    "\n",
    "if WHICH_DOC_TOKENIZER == \"IMPROVE\":\n",
    "  print(\"Improve tokenizer\")\n",
    "  nlp.tokenizer = improve_tokenizer(nlp=nlp)\n",
    "elif WHICH_DOC_TOKENIZER == \"UPDATE\":\n",
    "  print(\"Update tokenizer\")\n",
    "  nlp.tokenizer = update_tokenizer(nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "def save_to_disk(data, dir, filename):\n",
    "  db = DocBin()\n",
    "  for text, annotations in data:\n",
    "    doc = nlp(text)\n",
    "    # sentence_tokens = []\n",
    "    # for sent in doc.sents:\n",
    "    #   sentence_tokens.append([token.text for token in sent])\n",
    "    # print(sentence_tokens)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "      span = doc.char_span(int(start), int(end), label=label)\n",
    "      if not span == None:\n",
    "        ents.append(span)\n",
    "      else:\n",
    "        print(\"none\", int(start), int(end), label)\n",
    "    \n",
    "    # If two entities overlap, keep the one that covers more text\n",
    "    filtered_ents = []\n",
    "    for ent in sorted(ents, key=lambda e: (e.start, -len(e.text))):  # Sort by start index, prefer longer entities\n",
    "      if not any(ent.start < e.end and ent.end > e.start for e in filtered_ents):\n",
    "        filtered_ents.append(ent)\n",
    "    # print(ents)\n",
    "    doc.ents = filtered_ents\n",
    "    db.add(doc)\n",
    "\n",
    "  db.to_disk(os.path.join(dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_disk(training_data, os.path.join(path,'TrainDocBin'), \"improved_tokenizer_train.spacy\")\n",
    "save_to_disk(dev_data, os.path.join(path,'DevDocBin'), \"improved_tokenizer_dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "\n",
    "### STEP 2: prepare CUDA and PyTorch\n",
    "\n",
    "If your PC is already set up correctly, then skip to [step 3](#step3).\n",
    "\n",
    "#### Check if CUDA is available\n",
    "The instruction *torch.cuda.is_available()* checks if CUDA is avaiable for running the train on the GPU.\n",
    "If the answer if false, then it means either PyTorch or CUDA or both of them is not installed.\n",
    "\n",
    "#### Install PyTorch\n",
    "To install PyTorch, go to [this link](https://pytorch.org/get-started/locally/), select your preferences (in this case it is important to set a CUDA version as \"Compute Platform\" so that the code will run on the GPU) and then copy-paste the command into the following cell.\n",
    "\n",
    "It might be necessary to restart the runtime.\n",
    "\n",
    "After installing pythorch, *torch.cuda.is_available()* returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "import torch\n",
    "\n",
    "print('is cuda available? ', torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "\n",
    "### STEP 3: train the NER component\n",
    "\n",
    "#### Generate config.cfg file\n",
    "Generate the base_config.cfg configuration file that includes all the settings and hyperparameters.\n",
    "In this project the focus is to train only the ner component.\n",
    "The train will be optimized for accuracy over efficiency.\n",
    "Then, save the config to config.cfg file\n",
    "\n",
    "For this project the training is done with an NVIDIA GeForce 4060 laptop with 8GB of VRAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config ./config/trf/base_config.cfg ./config/trf/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variable `WHICH_TRAIN_TOKENIZER` to one of the possible values to use a particular tokenizer when training the model.\n",
    "\n",
    "Set the variable `WHICH_TRANSFORMER` to one of the possible values to use a particular transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH_TRAIN_TOKENIZER = \"IMPROVE\" # Possible values: DEFAULT, IMPROVE, UPDATE\n",
    "WHICH_TRANSFORMER = \"ROBERTA\" # Possible values: ROBERTA, BERT\n",
    "\n",
    "if WHICH_TRANSFORMER == \"ROBERTA\":\n",
    "  if WHICH_TRAIN_TOKENIZER == \"IMPROVE\":\n",
    "    print(\"train ROBERTA with IMPROVE tokenizer\")\n",
    "    !python -m spacy train ./config/trf/custom-config.cfg --output ./output/trf-train-improve-tokenizer-custom-config-new --paths.train ./TrainDocBin/new_improved_tokenizer_train.spacy --paths.dev ./DevDocBin/new_improved_tokenizer_dev.spacy --code ./improve-tokenizer.py --gpu-id 0\n",
    "  elif WHICH_TRAIN_TOKENIZER == \"UPDATE\":\n",
    "    print(\"train ROBERTA with UPDATE tokenizer\")\n",
    "    !python -m spacy train ./config/trf/custom-config.cfg --output ./output/trf-train-updated-tokenizer-custom-config --paths.train ./TrainDocBin/updated_tokenizer_train.spacy --paths.dev ./DevDocBin/updated_tokenizer_dev.spacy --code ./update-tokenizer.py --gpu-id 0\n",
    "  elif WHICH_TRAIN_TOKENIZER == \"DEFAULT\":\n",
    "    print(\"train ROBERTA with DEFAULT tokenizer\")\n",
    "    !python -m spacy train ./config/trf/config.cfg --output ./output/trf-all --paths.train ./TrainDocBin/all_train.spacy --paths.dev ./DevDocBin/all_dev.spacy --gpu-id 0\n",
    "elif WHICH_TRANSFORMER == \"BERT\":\n",
    "  if WHICH_TRAIN_TOKENIZER == \"IMPROVE\":\n",
    "    print(\"train BERT with IMPROVE tokenizer\")\n",
    "    !python -m spacy train ./config/trf/bert-custom-config.cfg --output ./output/trf-bert-improve-tokenizer-custom-config --paths.train ./TrainDocBin/improved_tokenizer_train.spacy --paths.dev ./DevDocBin/improved_tokenizer_dev.spacy --code ./improve-tokenizer.py --gpu-id 0\n",
    "  elif WHICH_TRAIN_TOKENIZER == \"UPDATE\":\n",
    "    print(\"train BERT with UPDATE tokenizer\")\n",
    "    !python -m spacy train ./config/trf/bert-custom-config.cfg --output ./output/trf-bert-update-tokenizer-custom-config --paths.train ./TrainDocBin/updated_tokenizer_train.spacy --paths.dev ./DevDocBin/updated_tokenizer_dev.spacy --code ./update-tokenizer.py --gpu-id 0\n",
    "  elif WHICH_TRAIN_TOKENIZER == \"DEFAULT\":\n",
    "    print(\"train BERT with DEFAULT tokenizer\")\n",
    "    !python -m spacy train ./config/trf/bert-custom-config.cfg --output ./output/trf-bert-custom-config --paths.train ./TrainDocBin/all_train.spacy --paths.dev ./DevDocBin/all_dev.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "\n",
    "for example in dev_data:\n",
    "\tentities = example[1]\n",
    "\tfor entity in entities:\n",
    "\t\tentity_label = entity[2]\n",
    "\t\tunique_labels.add(entity_label)\n",
    "        \n",
    "unique_labels_list = list(unique_labels)\n",
    "\n",
    "print('Entities to be recognised in the provided data: ')\n",
    "print(unique_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH_TRAIN_TOKENIZER = \"IMPROVE\" # Possible values: DEFAULT, IMPROVE, UPDATE\n",
    "WHICH_TRANSFORMER = \"ROBERTA\" # Possible values: ROBERTA, BERT\n",
    "\n",
    "if WHICH_TRAIN_TOKENIZER == \"IMPROVE\":\n",
    "  !spacy evaluate ./output/trf-train-improve-tokenizer-custom-config/model-best/ ./DevDocBin/improved_tokenizer_dev.spacy --code ./improve-tokenizer.py --gpu-id 0\n",
    "elif WHICH_TRAIN_TOKENIZER == \"UPDATE\":\n",
    "  !spacy evaluate ./output/trf-updated-tokenizer-custom-config/model-best/ ./DevDocBin/updated_tokenizer_dev.spacy --code ./update-tokenizer.py --gpu-id 0\n",
    "elif WHICH_TRAIN_TOKENIZER == \"DEFAULT\":\n",
    "  !spacy evaluate ./output/trf-updated-tokenizer-custom-config/model-best/ ./DevDocBin/all_dev.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy evaluate ./output/trf-blank-improve-nohtml-config/model-best/ ./DevDocBin/new_nohtml_dev.spacy --code ./improve-tokenizer.py --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy evaluate ./output/trf-train-improve-tokenizer-custom-config-new/model-best/ ./DevDocBin/old_improved_tokenizer_dev.spacy --code ./improve-tokenizer.py --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "\n",
    "### STEP 3 (off-topic): attach the trained NER comopnent to the original pipeline\n",
    "In the following Python cell, the trained NER component will be integrated into the original en_core_web_trf pipeline. This combination will allow the final pipeline to label words using a set that includes both the original labels and the newly trained ones, so that it can limit the \"Catastrophic Forgetting\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "# Load trained model\n",
    "trained_nlp = spacy.load('./output/path/to/model-best') # NOTE: set correct path\n",
    "\n",
    "trained_nlp.replace_listeners(\"transformer\", \"ner\", [\"model.tok2vec\"])\n",
    "\n",
    "nlp.add_pipe(\n",
    "  \"ner\",\n",
    "  name=\"trained_ner\",\n",
    "  source=trained_nlp,\n",
    "  before=\"ner\",\n",
    ")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.displacy\n",
    "\n",
    "\n",
    "doc = nlp(\"Doc here\")\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL: output of best pipeline\n",
    "\n",
    "In the following table they are shown the best combinations with their results, with a particular focus on precision (NER P), recall (NER R) and F1 score (NER F).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Pipeline | .spacy creation | Params | TOK | NER P | NER R | NER F |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| lg-all | en_core_web_lg | Default tokenizer | Default config + Default tokenizer | 99.90 | 81.62 | 72.73 | 76.92 |\n",
    "| trf-all | en_core_web_trf | Default tokenizer | Default config + Default tokenizer | 99.90 | **83.58** | 80.20 | 81.86 |\n",
    "| lg-improve-tokenizer | en_core_web_lg | Improved tokenizer | Default config + Default tokenizer | 99.74 | 78.29 | 74.57 | 76.39 |\n",
    "| trf-improve-tokenizer-custom-config | en_core_web_trf | Improved tokenizer | Custom config + Default tokenizer | 99.74 | 78.89 | **83.97** | 81.35 |\n",
    "| trf-improve-512-custom-config | en_core_web_trf | Improved tokenizer | Custom config + Improved tokenizer | 100.00 | 82.38 | 82.45 | 82.42 |\n",
    "| trf-updated-tokenizer-custom-config | en_core_web_trf | Updated tokenizer | Custom config + Default tokenizer | 90.71 | 80.98 | 81.56 | 81.27 |\n",
    "| lg-train-improve-tokenizer-custom-config | en_core_web_lg | Improved tokenizer | Custom config + Improved tokenizer | 100.00 | 79.78 | 77.71 | 78.73 |\n",
    "| trf-train-improve-tokenizer-custom-config | en_core_web_trf | Improved tokenizer | Custom config + Improved tokenizer | 100.00 | 82.47 | 82.54 | **82.51** |\n",
    "| trf-bert-improve-tokenizer-custom-config | en_core_web_trf with BERT | Improved tokenizer | Custom config + Improved tokenizer + BERT | 100.00 | 82.13 | 81.47 | 81.80 |\n",
    "| trf-nohtml-improve-tokenizer-custom-config | en_core_web_trf | Improved tokenizer + remove tags | Custom config + Improved tokenizer | 100.00 | 78.22 | 83.61 | 80.82 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ata-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
